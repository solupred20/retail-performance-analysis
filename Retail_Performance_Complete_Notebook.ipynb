{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f59fc29",
   "metadata": {},
   "source": [
    "# Retail Performance Analysis — Complete Notebook\n",
    "\n",
    "This notebook performs a full retail analytics pipeline on a sample Superstore-like dataset.\n",
    "\n",
    "### Files used (local paths)\n",
    "- Dataset (example): `/mnt/data/retail_project_outputs/synthetic_superstore.csv`\n",
    "- Project brief / ideas: `/mnt/data/data analyst projects.pdf`\n",
    "\n",
    "If you have your real dataset, replace the `DATA_PATH` variable below with your CSV path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2daad805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook outputs will be saved to \\mnt\\data\\retail_project_outputs\\notebook_outputs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths (edit DATA_PATH to point to your real dataset if needed)\n",
    "DATA_PATH = r\"C:\\Users\\sahha\\Downloads\\Retail_Performance_Pharma_Notebook.ipynb\"\n",
    "PROJECT_PDF = r\"/mnt/data/data analyst projects.pdf\"\n",
    "OUTPUT_DIR = Path('/mnt/data/retail_project_outputs/notebook_outputs')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Notebook outputs will be saved to', OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fc105",
   "metadata": {},
   "source": [
    "## 1) Load data\n",
    "\n",
    "Load the CSV and inspect the first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd6149e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing column provided to 'parse_dates': 'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# وبعدها أنشئ الأعمدة المتوقعة:\u001b[39;00m\n\u001b[32m      3\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mOrder Date\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\elevate final project\\retail_performance_analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\elevate final project\\retail_performance_analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\elevate final project\\retail_performance_analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\elevate final project\\retail_performance_analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\elevate final project\\retail_performance_analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:161\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m    155\u001b[39m         \u001b[38;5;28mself\u001b[39m._validate_usecols_names(\n\u001b[32m    156\u001b[39m             usecols,\n\u001b[32m    157\u001b[39m             \u001b[38;5;28mself\u001b[39m.names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    158\u001b[39m         )\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_parse_dates_presence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mself\u001b[39m._set_noconvert_columns()\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\elevate final project\\retail_performance_analysis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:243\u001b[39m, in \u001b[36mParserBase._validate_parse_dates_presence\u001b[39m\u001b[34m(self, columns)\u001b[39m\n\u001b[32m    233\u001b[39m missing_cols = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    234\u001b[39m     \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m    235\u001b[39m         {\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     )\n\u001b[32m    241\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    244\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing column provided to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mparse_dates\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m     )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Convert positions to actual column names\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    248\u001b[39m     col \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns) \u001b[38;5;28;01melse\u001b[39;00m columns[col]\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_needed\n\u001b[32m    250\u001b[39m ]\n",
      "\u001b[31mValueError\u001b[39m: Missing column provided to 'parse_dates': 'Date'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, parse_dates=['Date'], low_memory=False)\n",
    "# وبعدها أنشئ الأعمدة المتوقعة:\n",
    "df['Order Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "df['Ship Date'] = pd.NaT\n",
    "\n",
    "# قراءة الملف (مع تحويل عمود Date لتاريخ)\n",
    "DATA_PATH = r\"C:\\Users\\sahha\\Desktop\\New folder (3)\\pharmacy_sales_usa.csv\"  # عدّل المسار لو لازم\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=['Date'], low_memory=False)\n",
    "\n",
    "# اجعل العمود بنفس الاسم اللي النوتبوك متوقعه\n",
    "df['Order Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "# لو الكود بعد كده يتعامل مع Ship Date، نعرّف عمود فارغ (NaT) لتجنّب الأخطاء\n",
    "df['Ship Date'] = pd.NaT\n",
    "\n",
    "# عرض عيّنة للتأكد\n",
    "print(\"Columns now:\", list(df.columns))\n",
    "df.head()\n",
    "\n",
    "print('Rows, cols:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adeabef",
   "metadata": {},
   "source": [
    "## 2) Quick data quality checks\n",
    "\n",
    "Check columns, missing values and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns:', list(df.columns))\n",
    "print('\\nMissing values per column:')\n",
    "print(df.isna().sum())\n",
    "print('\\nNumber of duplicate rows:', df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fdc00f",
   "metadata": {},
   "source": [
    "## 3) Basic cleaning & feature engineering\n",
    "\n",
    "Create time features and profitability metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().copy()\n",
    "df['Year'] = df['Order Date'].dt.year\n",
    "df['Month'] = df['Order Date'].dt.month\n",
    "df['Quarter'] = df['Order Date'].dt.to_period('Q')\n",
    "df['Profit Margin'] = (df['Profit'] / df['Sales']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "df['Sales_per_unit'] = (df['Sales'] / df['Quantity']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "df['Discount_flag'] = (df['Discount'] > 0).astype(int)\n",
    "print('After cleaning — rows, cols:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb47ee",
   "metadata": {},
   "source": [
    "## 4) KPI calculations\n",
    "\n",
    "Compute a small KPI table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45082370",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_revenue = df['Sales'].sum()\n",
    "total_profit = df['Profit'].sum()\n",
    "avg_margin = (total_profit / total_revenue) if total_revenue else 0\n",
    "num_orders = df['Order ID'].nunique() if 'Order ID' in df.columns else len(df)\n",
    "num_products = df['Product Name'].nunique() if 'Product Name' in df.columns else 0\n",
    "kpis = {\n",
    "    'Total Revenue': total_revenue,\n",
    "    'Total Profit': total_profit,\n",
    "    'Avg Profit Margin': avg_margin,\n",
    "    'Num Orders': num_orders,\n",
    "    'Num Products': num_products\n",
    "}\n",
    "kpis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9d7d6",
   "metadata": {},
   "source": [
    "## 5) Monthly sales & profit trend\n",
    "\n",
    "Plot monthly sales and profit series and save images to OUTPUT_DIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e89fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly = df.groupby(pd.Grouper(key='Order Date', freq='M'))[['Sales','Profit']].sum().reset_index().sort_values('Order Date')\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(monthly['Order Date'], monthly['Sales'], marker='o')\n",
    "plt.title('Monthly Sales Trend')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Sales')\n",
    "plt.tight_layout()\n",
    "monthly_sales_png = OUTPUT_DIR / 'monthly_sales.png'\n",
    "plt.savefig(str(monthly_sales_png))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(monthly['Order Date'], monthly['Profit'], marker='o')\n",
    "plt.title('Monthly Profit Trend')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Profit')\n",
    "plt.tight_layout()\n",
    "monthly_profit_png = OUTPUT_DIR / 'monthly_profit.png'\n",
    "plt.savefig(str(monthly_profit_png))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0bfea",
   "metadata": {},
   "source": [
    "## 6) Category sales\n",
    "\n",
    "Bar chart of sales by Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c681930",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sales = df.groupby('Category')['Sales'].sum().sort_values(ascending=False)\n",
    "plt.figure(figsize=(6,4))\n",
    "cat_sales.plot(kind='bar')\n",
    "plt.title('Sales by Category')\n",
    "plt.ylabel('Sales')\n",
    "plt.tight_layout()\n",
    "category_sales_png = OUTPUT_DIR / 'category_sales.png'\n",
    "plt.savefig(str(category_sales_png))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949c5f1",
   "metadata": {},
   "source": [
    "## 7) Discount vs Profit scatter (sample)\n",
    "\n",
    "Scatter to visualise the relationship between discount and profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98240636",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n=min(1200, len(df)), random_state=1)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(sample['Discount'], sample['Profit'], alpha=0.6, s=10)\n",
    "plt.title('Discount vs Profit (sample)')\n",
    "plt.xlabel('Discount')\n",
    "plt.ylabel('Profit')\n",
    "plt.tight_layout()\n",
    "disc_profit_png = OUTPUT_DIR / 'discount_vs_profit.png'\n",
    "plt.savefig(str(disc_profit_png))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61f547",
   "metadata": {},
   "source": [
    "## 8) Top products\n",
    "\n",
    "Horizontal bar of top 10 products by sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595203e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products = df.groupby('Product Name')['Sales'].sum().sort_values(ascending=False).head(10)\n",
    "plt.figure(figsize=(8,4))\n",
    "top_products.plot(kind='barh')\n",
    "plt.title('Top 10 Products by Sales')\n",
    "plt.xlabel('Sales')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "top_products_png = OUTPUT_DIR / 'top_products.png'\n",
    "plt.savefig(str(top_products_png))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35093f15",
   "metadata": {},
   "source": [
    "## 9) Region performance\n",
    "\n",
    "Summarise sales and profit by region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_perf = df.groupby('Region')[['Sales','Profit']].sum().sort_values('Sales', ascending=False)\n",
    "region_perf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b96fa",
   "metadata": {},
   "source": [
    "## 10) Summary exports\n",
    "\n",
    "Save summary CSVs and list files created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461faaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.groupby('Category').agg({'Sales':'sum','Profit':'sum','Quantity':'sum'}).reset_index()\n",
    "summary['Profit Margin'] = summary['Profit'] / summary['Sales']\n",
    "summary_csv = OUTPUT_DIR / 'category_summary.csv'\n",
    "summary.to_csv(summary_csv, index=False)\n",
    "print('Saved', summary_csv)\n",
    "print('Saved images:')\n",
    "for p in ['monthly_sales.png','monthly_profit.png','category_sales.png','discount_vs_profit.png','top_products.png']:\n",
    "    print('-', OUTPUT_DIR / p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8b768",
   "metadata": {},
   "source": [
    "## Next steps / notes\n",
    "- Replace `DATA_PATH` with your real dataset to reproduce results on real data.\n",
    "- The project brief PDF is at the local path:\n",
    "  - `/mnt/data/data analyst projects.pdf`\n",
    "- Use the generated images in `/mnt/data/retail_project_outputs/notebook_outputs` for README or PPTX.\n",
    "- If you want, copy this notebook into your repo `notebooks/` folder and run it in Jupyter to produce outputs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
